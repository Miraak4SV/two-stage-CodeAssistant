import os
import streamlit as st
from sentence_transformers import SentenceTransformer, util
import json
import torch
import requests
import ast

# --- Constants ---
MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
TOP_K = 3
BASE_FILE = 'sklearn_knowledge_base.jsonl'
EMBEDDINGS_CACHE = 'embeddings.pt'
MAX_PROMPT_LINES = 50  # –º–∞–∫—Å. —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ –ø—Ä–æ–º–ø—Ç–µ

# --- Streamlit UI ---
st.title('Code Assistant for Scikit-learn')

# --- Retrieve API Key (manual input) ---
API_KEY = st.text_input('–í–≤–µ–¥–∏—Ç–µ YandexGPT API-–∫–ª—é—á', type='password')
if not API_KEY:
    st.warning('–í–≤–µ–¥–∏—Ç–µ API-–∫–ª—é—á –≤ –ø–æ–ª–µ –≤—ã—à–µ')
    st.stop()

mode = st.radio('–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞', ['Semantic', 'Grep'])
user_query = st.text_input('–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å')

# --- Load articles ---
@st.cache_data
def load_articles(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [json.loads(line) for line in f]

articles = load_articles(BASE_FILE)
texts = [art['content'] + ' ' + ' '.join(art.get('queries', [])) for art in articles]

# --- Load model ---
@st.cache_resource
def load_model():
    return SentenceTransformer(MODEL_NAME)

model = load_model()

# --- Encode or load embeddings ---
@st.cache_data
def load_embeddings(texts):
    if os.path.exists(EMBEDDINGS_CACHE):
        return torch.load(EMBEDDINGS_CACHE)
    embeddings = model.encode(texts, convert_to_tensor=True)
    torch.save(embeddings, EMBEDDINGS_CACHE)
    return embeddings

article_embeddings = load_embeddings(texts)

# --- Grep search ---
def grep_search(keyword, articles, top_k=TOP_K):
    keyword = keyword.lower()
    hits = []
    for art in articles:
        cnt = art['content'].lower().count(keyword)
        if cnt > 0:
            hits.append((art, cnt))
    hits.sort(key=lambda x: x[1], reverse=True)
    return [art for art,_ in hits[:top_k]]

# --- Prompt generation ---
def trim_content(content):
    lines = content.splitlines()
    if len(lines) > MAX_PROMPT_LINES:
        return '\n'.join(lines[:MAX_PROMPT_LINES]) + '\n#... (truncated)'
    return content


def generate_prompt(article, question):
    snippet = trim_content(article['content'])
    return f"""–ö–æ–¥ ({article.get('path','<unknown>')}):
{snippet}

–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:
–≠—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤—ã–±—Ä–∞–Ω, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å: \"{question}\"

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{question}"""

# --- Call YandexGPT ---
def call_yandex_gpt(prompt):
    headers = {'Authorization': f'Bearer {API_KEY}', 'Content-Type': 'application/json'}
    data = {
        'modelUri': 'gpt://b1gsbr4nrvr3j95f6nqp/yandexgpt-lite',
        'completionOptions': {'stream': False, 'temperature': 0.6, 'maxTokens': '2000'},
        'messages': [
            {'role': 'system', 'text': '–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞.'},
            {'role': 'user', 'text': prompt}
        ]
    }
    resp = requests.post(
        'https://llm.api.cloud.yandex.net/foundationModels/v1/completion',
        headers=headers, json=data
    )
    if resp.status_code == 200:
        return resp.json().get('result', {}).get('alternatives', [{}])[0].get('message', {}).get('text', '')
    else:
        st.error(f'–û—à–∏–±–∫–∞ API: {resp.status_code} {resp.text}')
        return None

# --- Main logic ---
if user_query:
    # –ø–µ—Ä–≤–∏—á–Ω—ã–π –ø–æ–∏—Å–∫
    if mode == 'Grep' and user_query.lower().startswith('grep:'):
        key = user_query.split(':',1)[1].strip()
        top = grep_search(key, articles)
        st.write(f'üîé Grep-—Ä–µ–∂–∏–º: –Ω–∞–π–¥–µ–Ω–æ {len(top)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø–æ "{key}"')
    else:
        q_emb = model.encode(user_query, convert_to_tensor=True)
        scores = util.pytorch_cos_sim(q_emb, article_embeddings)[0]
        idxs = torch.topk(scores, k=TOP_K).indices
        top = [articles[i] for i in idxs]
        st.write(f'üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º: —Ç–æ–ø {len(top)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤')
    
    # –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–Ω–∏–ø–ø–µ—Ç—ã
    for i, art in enumerate(top, 1):
        st.subheader(f"{i}. {art['title']} [{art.get('path','')}] ")
        st.code(trim_content(art['content']), language='python')
    
    # —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ –ø–µ—Ä–≤–æ–º—É —Å–Ω–∏–ø–ø–µ—Ç—É (–¥–ª—è grep-—Ä–µ–∂–∏–º–∞)
    if mode == 'Grep':
        followup = st.text_input('–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É', '')
    else:
        followup = user_query
    
    if followup:
        prompt = generate_prompt(top[0], followup)
        st.markdown('**–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç:**')
        st.text_area('Prompt', prompt, height=200)
        if st.button('–û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ YandexGPT'):
            with st.spinner('–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞...'):
                answer = call_yandex_gpt(prompt)
            if answer:
                st.subheader('–û—Ç–≤–µ—Ç YandexGPT')
                st.write(answer)
            else:
                st.error('–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏.')
